---

  - name: Fetch container images before upgrade
    shell: |
      kubectl get pods -A -o jsonpath="{range .items[*].spec.containers[*]}{.image}{'\n'}{end}" |
      sort | uniq > /tmp/manifests/container_images_before_upgrade.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
#                       Upgrade controlplane components                                 |
# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------

  - name: Backup secrets for re-use when pods are re-created during the upgrade process
    shell: |
           kubectl get secrets -n {{ NAMEPREFIX }}-system -o json |
           jq '.items[]|del(.metadata|.managedFields,.uid,.resourceVersion)' > /tmp/secrets.with.values.yaml
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Cleanup - remove existing next versions of controlplane components CRDs
    vars:
      working_dir: "{{HOME}}/.cluster-api/dev-repository"
    file:
      state: absent
      path: "{{item}}"
    with_items:
    - "{{working_dir}}/cluster-api/{{CAPI_REL_TO_VERSION}}/"
    - "{{working_dir}}/bootstrap-kubeadm/{{CAPI_REL_TO_VERSION}}"
    - "{{working_dir}}/control-plane-kubeadm/{{CAPI_REL_TO_VERSION}}"
    - "{{working_dir}}/infrastructure-metal3/{{CAPM3_REL_TO_VERSION}}"

  - name: Generate clusterctl configuration file
    ansible.builtin.template:
      src: clusterctl-upgrade-test.yaml
      dest: "{{HOME}}/.cluster-api/clusterctl.yaml"

  - name: Get clusterctl repo
    ansible.builtin.git:
      repo: 'https://github.com/kubernetes-sigs/cluster-api.git'
      dest: /tmp/cluster-api-clone
      version: "{{ CAPI_REL_TO_VERSION }}"

  - name: Checkout upgraded version of capm3
    ansible.builtin.git:
      repo: 'https://github.com/metal3-io/cluster-api-provider-metal3.git'
      dest: "{{ CAPM3PATH }}"
      version: "{{ upgraded_capm3_branch }}"
      # No need to clone, it is already available
      clone: no
      # The repo is modified as part of deploying capm3 so we must force
      force: yes

  - name: Create clusterctl-settings.json for cluster-api and capm3 repos
    ansible.builtin.template:
      src: "{{ item.src }}"
      dest: "{{ item.dest }}"
    loop:
      - src: cluster-api-clusterctl-settings.json
        dest: /tmp/cluster-api-clone/clusterctl-settings.json
      - src: capm3-clusterctl-settings.json
        dest: "{{ CAPM3PATH }}/clusterctl-settings.json"

  - name: Build clusterctl binary
    ansible.builtin.command: make clusterctl
    args:
      chdir: /tmp/cluster-api-clone/

  - name: Copy clusterctl to /usr/local/bin
    ansible.builtin.copy:
      src: /tmp/cluster-api-clone/bin/clusterctl
      dest: /usr/local/bin/clusterctl
      remote_src: yes
      owner: root
      mode: u=rwx,g=rx,o=rx
    become: yes
    become_user: root

  - name: Create local repository
    ansible.builtin.command: cmd/clusterctl/hack/create-local-repository.py
    args:
      chdir: /tmp/cluster-api-clone/
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update kustomizations with images to use
    ansible.builtin.command: "{{ item }}"
    args:
      chdir: "{{ CAPM3PATH }}"
    environment:
      MANIFEST_IMG: "{{ upgraded_capm3_image }}"
      MANIFEST_TAG: "{{ upgraded_capm3_tag }}"
      MANIFEST_IMG_IPAM: "{{ upgraded_ipam_image }}"
      MANIFEST_TAG_IPAM: "{{ upgraded_ipam_tag }}"
    loop:
      - make set-manifest-image
      - make set-manifest-image-ipam

  - name: Build IPAM manifests
    ansible.builtin.command: "{{ item }}"
    args:
      chdir: "{{ CAPM3PATH }}"
    loop:
      - make hack/tools/bin/kustomize
      - ./hack/tools/bin/kustomize build "{{ IPAMPATH }}/config/default" --output config/ipam/metal3-ipam-components.yaml

  - name: Update CAPM3 imports to use local IPAM manifest
    ansible.builtin.replace:
      path: "{{ CAPM3PATH }}/config/ipam/kustomization.yaml"
      regexp: https://github\.com/metal3-io/ip-address-manager/releases/download/v.*/ipam-components\.yaml
      replace: metal3-ipam-components.yaml

  - name: Make capm3 release manifests
    ansible.builtin.command: make release-manifests
    args:
      chdir: "{{ CAPM3PATH }}"

  - name: Create folder structure for next version controlplane components
    vars:
      working_dir: "{{HOME}}/.cluster-api"
    file:
      path: "{{ working_dir }}/{{ item }}"
      state: directory
      recurse: yes
    with_items:
    - dev-repository/cluster-api/{{ CAPIRELEASE }}
    - dev-repository/bootstrap-kubeadm/{{ CAPIRELEASE }}
    - dev-repository/control-plane-kubeadm/{{ CAPIRELEASE }}
    - dev-repository/cluster-api/{{ CAPI_REL_TO_VERSION }}
    - dev-repository/bootstrap-kubeadm/{{ CAPI_REL_TO_VERSION }}
    - dev-repository/control-plane-kubeadm/{{ CAPI_REL_TO_VERSION }}
    - overrides/infrastructure-metal3/{{ CAPM3RELEASE }}
    - overrides/infrastructure-metal3/{{ CAPM3_REL_TO_VERSION }}

  - name: Copy controlplane components files
    vars:
      working_dir: "{{HOME}}/.cluster-api"
    copy: src="{{working_dir}}/{{item.src}}" dest="{{working_dir}}/{{item.dest}}"
    with_items:
    - {
        src: "dev-repository/cluster-api/{{CAPIRELEASE_HARDCODED}}/core-components.yaml",
        dest: "dev-repository/cluster-api/{{CAPIRELEASE}}/core-components.yaml"
      }
    - {
        src: "dev-repository/cluster-api/{{CAPIRELEASE_HARDCODED}}/metadata.yaml",
        dest: "dev-repository/cluster-api/{{CAPIRELEASE}}/metadata.yaml"
      }
    - {
        src: "dev-repository/bootstrap-kubeadm/{{CAPIRELEASE_HARDCODED}}/bootstrap-components.yaml",
        dest: "dev-repository/bootstrap-kubeadm/{{CAPIRELEASE}}/bootstrap-components.yaml"
      }
    - {
        src: "dev-repository/bootstrap-kubeadm/{{CAPIRELEASE_HARDCODED}}/metadata.yaml",
        dest: "dev-repository/bootstrap-kubeadm/{{CAPIRELEASE}}/metadata.yaml"
      }
    - {
        src: "dev-repository/control-plane-kubeadm/{{CAPIRELEASE_HARDCODED}}/control-plane-components.yaml",
        dest: "dev-repository/control-plane-kubeadm/{{CAPIRELEASE}}/control-plane-components.yaml"
      }
    - {
        src: "dev-repository/control-plane-kubeadm/{{CAPIRELEASE_HARDCODED}}/metadata.yaml",
        dest: "dev-repository/control-plane-kubeadm/{{CAPIRELEASE}}/metadata.yaml"
      }

  - name: Remove clusterctl generated next version of controlplane components folders
    vars:
      working_dir: "{{HOME}}/.cluster-api/dev-repository"
    file:
      state: absent
      path: "{{item}}"
    with_items:
    - "{{working_dir}}/cluster-api/{{CAPIRELEASE_HARDCODED}}"
    - "{{working_dir}}/bootstrap-kubeadm/{{CAPIRELEASE_HARDCODED}}"
    - "{{working_dir}}/control-plane-kubeadm/{{CAPIRELEASE_HARDCODED}}"

  - name: Create next version controller CRDs
    vars:
      dev_repository: "{{HOME}}/.cluster-api/dev-repository"
      overrides: "{{HOME}}/.cluster-api/overrides"
    copy:
      src: "{{item.src}}"
      dest: "{{item.dest}}"
    loop:
    - src: "{{ dev_repository }}/cluster-api/{{CAPIRELEASE}}/core-components.yaml"
      dest: "{{ dev_repository }}/cluster-api/{{CAPI_REL_TO_VERSION}}/core-components.yaml"
    - src: "{{ dev_repository }}/cluster-api/{{CAPIRELEASE}}/metadata.yaml"
      dest: "{{ dev_repository }}/cluster-api/{{CAPI_REL_TO_VERSION}}/metadata.yaml"
    - src: "{{ dev_repository }}/bootstrap-kubeadm/{{CAPIRELEASE}}/bootstrap-components.yaml"
      dest: "{{ dev_repository }}/bootstrap-kubeadm/{{CAPI_REL_TO_VERSION}}/bootstrap-components.yaml"
    - src: "{{ dev_repository }}/bootstrap-kubeadm/{{CAPIRELEASE}}/metadata.yaml"
      dest: "{{ dev_repository }}/bootstrap-kubeadm/{{CAPI_REL_TO_VERSION}}/metadata.yaml"
    - src: "{{ dev_repository }}/control-plane-kubeadm/{{CAPIRELEASE}}/control-plane-components.yaml"
      dest: "{{ dev_repository }}/control-plane-kubeadm/{{CAPI_REL_TO_VERSION}}/control-plane-components.yaml"
    - src: "{{ dev_repository }}/control-plane-kubeadm/{{CAPIRELEASE}}/metadata.yaml"
      dest: "{{ dev_repository }}/control-plane-kubeadm/{{CAPI_REL_TO_VERSION}}/metadata.yaml"

    - src: "{{ CAPM3PATH }}/out/infrastructure-components.yaml"
      dest: "{{ overrides }}/infrastructure-metal3/{{ CAPM3_REL_TO_VERSION }}/infrastructure-components.yaml"
    - src: "{{ CAPM3PATH }}/out/metadata.yaml"
      dest: "{{ overrides }}/infrastructure-metal3/{{ CAPM3_REL_TO_VERSION }}/metadata.yaml"

  - name: Perform upgrade on the target cluster
    ansible.builtin.command: clusterctl upgrade apply --contract v1beta1
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  # TODO: Can we check this somehow instead of just waiting?
  # Relevant issue: https://github.com/kubernetes-sigs/cluster-api/issues/4474
  - name: Wait for upgrade on the target cluster
    ansible.builtin.pause:
      seconds: 30

  - name: Restore secrets to fill missing secret fields after performing target cluster upgrade
    ansible.builtin.command: kubectl replace -f /tmp/secrets.with.values.yaml
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Perform upgrade on the source cluster
    ansible.builtin.command: clusterctl upgrade apply --contract v1beta1

  # TODO: Can we check this somehow instead of just waiting?
  # Relevant issue: https://github.com/kubernetes-sigs/cluster-api/issues/4474
  - name: Wait for upgrade on the source cluster
    ansible.builtin.pause:
      seconds: 30

  - name: Verify that CP components are updated and available
    k8s_info:
      api_version: v1
      kind: Deployment
      label_selectors:
        - clusterctl.cluster.x-k8s.io
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: controller_deployments
    retries: 200
    delay: 20
    vars:
      # Select any item where updatedReplicas or availableReplicas is different from replicas.
      # This would indicate that there are some unhealthy pods or a stuck/failed rollout.
      query: "[?(status.updatedReplicas != status.replicas) || (status.availableReplicas != status.replicas)]"
    until: (controller_deployments is succeeded) and
           (controller_deployments.resources | json_query(query) | length == 0)

  - name: Verify upgraded API resources for CAPI and CAPM3
    kubernetes.core.k8s_cluster_info:
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: api_status
    failed_when: ('cluster.x-k8s.io/' + upgraded_capi_version not in api_status.apis) or
                 ('controlplane.cluster.x-k8s.io/' + upgraded_capi_version not in api_status.apis) or
                 ('bootstrap.cluster.x-k8s.io/' + upgraded_capi_version not in api_status.apis) or
                 ('infrastructure.cluster.x-k8s.io/' + UPGRADED_CAPM3_VERSION not in api_status.apis)

# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
#                       Upgrade Ironic                                                  |
# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------

  - name: Set expected ironic image based containers
    set_fact:
      ironic_image_containers:
        - ironic
        - ironic-dnsmasq
        - ironic-httpd
        - ironic-log-watch
        - ironic-inspector
        # There is also a keepalived container in the pods, but it is using a
        # different image than the rest and therefore not included in the list.
        # - ironic-endpoint-keepalived

  - name: Upgrade ironic image based containers
    kubernetes.core.k8s:
      api_version: v1
      kind: Deployment
      name: "{{ NAMEPREFIX }}-ironic"
      namespace: "{{ IRONIC_NAMESPACE }}"
      resource_definition:
        spec:
          template:
            spec:
              containers: "{{ ironic_based_containers + mariadb_container }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    vars:
      # Generate a list of name/image pairs from the ironic_image_containers.
      # This is to avoid looping which would create one new revision for each container.
      # 1. Zip to get a list of lists: [[container_name, image_name], [...]]
      #    (all have the same image)
      # 2. Turn it into a dict so we have {container_name: image_name, ...}
      # 3. Convert it to a list of {name: container_name, image: image_name}
      ironic_based_containers:
        "{{ dict(ironic_image_containers |
              zip_longest([], fillvalue=CONTAINER_REGISTRY+'/metal3-io/ironic:'+IRONIC_IMAGE_TAG)) |
            dict2items(key_name='name', value_name='image') }}"
      mariadb_container:
      - name:  mariadb
        image: "{{CONTAINER_REGISTRY+'/metal3-io/mariadb:'+MARIADB_IMAGE_TAG}}"


  - name: Wait for ironic update to rollout
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Deployment
      name: "{{ NAMEPREFIX }}-ironic"
      namespace: "{{ IRONIC_NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 100
    delay: 10
    register: ironic_deployment
    # We are checking that there is 1 updated replica, that it is available, and
    # that it is the only one (so no old replica left).
    # Note that the these fields can be missing if the controller didn't have
    # time to update them yet, so we need to set a default value.
    until: (ironic_deployment is succeeded) and
           (ironic_deployment.resources | length > 0) and
           (ironic_deployment.resources[0].status.updatedReplicas | default(0) == 1) and
           (ironic_deployment.resources[0].status.availableReplicas | default(0) == 1) and
           (ironic_deployment.resources[0].status.replicas | default(0) == 1)

  - name: Fetch container images after upgrade
    shell: |
      kubectl get pods -A -o jsonpath="{range .items[*].spec.containers[*]}{.image}{'\n'}{end}" |
      sort | uniq > /tmp/manifests/container_images_after_upgrade.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
#                       Upgrade K8S version and boot-image                              |
# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
  - name: Download upgraded boot-disk image for deployment
    include_tasks: download_image.yml
    vars:
      IMAGE_NAME: "UBUNTU_20.04_NODE_IMAGE_K8S_{{UPGRADED_K8S_VERSION}}.qcow2"
      RAW_IMAGE_NAME: "UBUNTU_20.04_NODE_IMAGE_K8S_{{UPGRADED_K8S_VERSION}}-raw.img"
      IMAGE_LOCATION: "https://artifactory.nordix.org/artifactory/metal3/images/k8s_{{UPGRADED_K8S_VERSION}}"
      IMAGE_URL: "http://172.22.0.1/images/{{ RAW_IMAGE_NAME }}"
      IMAGE_CHECKSUM: "http://172.22.0.1/images/{{ RAW_IMAGE_NAME }}.{{ IMAGE_CHECKSUM_TYPE }}sum"

  - name: Get cluster uid
    kubernetes.core.k8s_info:
      api_version: cluster.x-k8s.io/{{ upgraded_capi_version }}
      kind: Cluster
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: clusters

  - name: Create controlplane Metal3MachineTemplates
    kubernetes.core.k8s:
      state: present
      template: Metal3MachineTemplate.yml
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    vars:
      CLUSTER_UID: "{{ clusters.resources[0].metadata.uid }}"
      M3MT_NAME: "{{CLUSTER_NAME}}-new-controlplane-image"
      DATA_TEMPLATE_NAME: "{{CLUSTER_NAME}}-controlplane-template"
      RAW_IMAGE_NAME: "UBUNTU_20.04_NODE_IMAGE_K8S_{{UPGRADED_K8S_VERSION}}-raw.img"
      IMAGE_URL: "http://172.22.0.1/images/{{ RAW_IMAGE_NAME }}"
      IMAGE_CHECKSUM: "http://172.22.0.1/images/{{ RAW_IMAGE_NAME }}.{{ IMAGE_CHECKSUM_TYPE }}sum"
      NODE_REUSE_STATUS: "false"
      CAPI_VERSION: "{{ upgraded_capi_version }}"
      CAPM3_VERSION: "{{ UPGRADED_CAPM3_VERSION }}"

  - name: Create worker Metal3MachineTemplates
    kubernetes.core.k8s:
      state: present
      template: Metal3MachineTemplate.yml
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    vars:
      CLUSTER_UID: "{{ clusters.resources[0].metadata.uid }}"
      M3MT_NAME: "{{CLUSTER_NAME}}-new-workers-image"
      DATA_TEMPLATE_NAME: "{{CLUSTER_NAME}}-workers-template"
      RAW_IMAGE_NAME: "UBUNTU_20.04_NODE_IMAGE_K8S_{{UPGRADED_K8S_VERSION}}-raw.img"
      IMAGE_URL: "http://172.22.0.1/images/{{ RAW_IMAGE_NAME }}"
      IMAGE_CHECKSUM: "http://172.22.0.1/images/{{ RAW_IMAGE_NAME }}.{{ IMAGE_CHECKSUM_TYPE }}sum"
      NODE_REUSE_STATUS: "false"
      CAPI_VERSION: "{{ upgraded_capi_version }}"
      CAPM3_VERSION: "{{ UPGRADED_CAPM3_VERSION }}"

  - name: Update boot-disk and kubernetes versions of controlplane nodes
    kubernetes.core.k8s:
      api_version: controlplane.cluster.x-k8s.io/{{ upgraded_capi_version }}
      kind: KubeadmControlPlane
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      resource_definition:
        spec:
          version: "{{UPGRADED_K8S_VERSION}}"
          machineTemplate:
            infrastructureRef:
              name: "{{CLUSTER_NAME}}-new-controlplane-image"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Verify that controlplane nodes using the new node image
    shell: |
            kubectl get bmh -n {{NAMESPACE}} |
            grep -i provisioned | grep -c 'new-controlplane-image'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: new_image_cp_nodes
    until: new_image_cp_nodes.stdout|int == 3
    failed_when: new_image_cp_nodes.stdout|int != 3

  - name: Untaint all CP nodes after upgrade of controlplane nodes
    shell: |
        kubectl taint nodes --all node-role.kubernetes.io/control-plane-
        kubectl taint nodes --all node-role.kubernetes.io/master-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Verify number of new control plane machines
    shell: |
            kubectl get machines -n "{{ NAMESPACE }}" -l cluster.x-k8s.io/control-plane -o json |
            jq -r '[ .items[] | select(.spec.version == "{{ UPGRADED_K8S_VERSION }}") | .status.nodeRef.name ] | length'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: new_control_plane_machines
    until: new_control_plane_machines.stdout|int == 3
    failed_when: new_control_plane_machines.stdout|int != 3

  - name: Register worker nodes
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Node
      label_selectors:
        - "!node-role.kubernetes.io/control-plane"
        - "!node-role.kubernetes.io/master"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: worker_nodes

  - name: Label worker for scheduling purpose
    kubernetes.core.k8s:
      api_version: v1
      kind: Node
      name: "{{ worker_nodes.resources[0].metadata.name }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
      resource_definition:
        metadata:
          labels:
            type: worker

  - name: Deploy workload with nodeAffinity
    kubernetes.core.k8s:
      state: present
      resource_definition: "{{ lookup('file', 'workload.yaml') | from_yaml }}"
      namespace: default
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
      wait: yes
    register: workload

  - name: Show workload deployment status
    debug:
      msg: "{{ workload }}"

  - name: Verify workload deployment
    kubernetes.core.k8s_info:
      api_version: apps/v1
      kind: Deployment
      name: workload-1-deployment
      namespace: default
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 3
    delay: 20
    register: workload_pods
    until: (workload_pods is succeeded) and
           (workload_pods.resources | length > 0) and
           (workload_pods.resources[0].status.readyReplicas == workload_pods.resources[0].spec.replicas)

  - name: Update MachineDeployment maxSurge and maxUnavailable fields
    kubernetes.core.k8s:
      api_version: cluster.x-k8s.io/{{ upgraded_capi_version }}
      kind: MachineDeployment
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
      resource_definition:
        spec:
          strategy:
            rollingUpdate:
              maxSurge: 1
              maxUnavailable: 1

  - name: Update boot-disk and kubernetes versions of worker node
    kubernetes.core.k8s:
      api_version: cluster.x-k8s.io/{{ upgraded_capi_version }}
      kind: MachineDeployment
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
      resource_definition:
        spec:
          template:
            spec:
              version: "{{ UPGRADED_K8S_VERSION }}"
              infrastructureRef:
                name: "{{ CLUSTER_NAME }}-new-workers-image"

  - name: Verify that worker node is using the new boot-image
    kubernetes.core.k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    vars:
      query: "[? (status.provisioning.state=='provisioned') &&
                 (starts_with(spec.consumerRef.name, '{{CLUSTER_NAME}}-new-workers-image'))]"
    register: bmh
    retries: 200
    delay: 20
    until: (bmh is succeeded) and
           (bmh.resources | length > 0) and
           (bmh.resources | json_query(query) | length == 1)

  - name: Verify that the upgraded worker node has joined the cluster
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Node
      label_selectors:
        - "!node-role.kubernetes.io/control-plane"
        - "!node-role.kubernetes.io/master"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: worker_nodes
    until: (worker_nodes is succeeded) and
           (worker_nodes.resources | length == 1)

  - name: Verify that kubernetes version is upgraded for CP and worker nodes
    kubernetes.core.k8s_info:
      api_version: cluster.x-k8s.io/{{ upgraded_capi_version }}
      kind: Machine
      namespace: "{{ NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: machines
    failed_when: (machines.resources | map(attribute='spec.version') | unique | length != 1) or
                 (machines.resources | map(attribute='spec.version') | first != "{{ UPGRADED_K8S_VERSION }}")

# Before pivoting back we must ensure all machines are provisioned and running.
  - name: Verify that all machines are provisioned and running.
    include_tasks: verify_resources_states.yml
    vars:
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
